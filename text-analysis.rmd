---
title: "Text analysis in R"
author: "Scott Bailey"
date: "7/23/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(tidytext)
library(readtext)
```

The texts we're going to work with are a set of interviews from here at Hopkins. You can download them from here: 

During our workshop, we're going to work through a standard workflow in text analysis: read in texts, organize them, clean them, and analyze them.

We'll use a special package, `readtext`, to read the texts in by passing in the directory.

```{r}
interviews_raw <- readtext("plain_text")

head(interviews_raw)
```

To understanding how we need to clean the texts, we need to take a close look at one of them. One of the first things we can see is that the text is extraordinarily clean. It's re-keyed text or a corrected transcript, rather than OCR, so we don't need to clean or handle misspellings or strange characters having come in by way of the OCDR process. 

We should also notice that there is front-matter to teach text file, with information about the interview. If we look at several of these, we'll see that they take a different number of lines in different files, which is important.If we're wanting to do semantic analysis of the text, though, we probably want to pull out the front matter, though if you have a large enough corpus some of that information would wash out in the analysis. 

The last thing to observe is that, for the most part, each line in the file represents a switch in person speaking, and each line has the person speaking identified at the front of the line. This isn't always the case, but it helps we want to analyze these files in terms of what the interview says vs what the interviewee says. 

Let's start by splitting the front-matter from the interview itself. Can anyone suggest a way to do this?

```{r}
interviews <- interviews_raw %>%
  separate(text, c("series_title", "front_matter", "text"), "\n\n", extra="merge")

head(interviews)
```

Separate allows us to split a column into multiple columns based on some character pattern. We have to specific the `extra` parameter so that we control what happens when separate would create more columns than we want, i.e., when there are more characters that satisfy our pattern than we want.

Now we can do a bit of cleaning. Let's remove the word "INTERVIEW" since that doesn't meaningfully contribute to our analysis. 

```{r}
interviews <- interviews %>%
  mutate(text = str_replace_all(text, "INTERVIEW", ""))

head(interviews)
```

Since the Bill Austin interview is notes from the interview rather than a transcript, let's drop it from our analysis. We want our documents to all be roughly the same in format. 


```{r}
interviews <- interviews %>%
  filter(!doc_id == "AustinBill_phoneInterview.txt")

head(interviews)
```

Now that we have our data mostly standardized, let's start asking some questions about it. 

How many times is Hopkins mentioned in each interview?

```{r}
interviews %>%
  pull(text) %>% # pull extracts just that column vector
  str_count("Hopkins")
```

We got back a vector here, so we could assign that to a variable and do whatever calculations on it we want to. Or, we could create a new column in our dataset with that information for any words we are particularly interested in.

Let's do something smiilar, but with how many words are in each interview.

```{r}
interviews <- interviews %>%
  mutate(n_words = str_count(text, boundary("word")))

head(interviews)
```

The `boundary` parameter lets us choose some time of entity to count by. Along with words, it can count thinks like characters, sentences, and line breaks. 

There are quite a function string functions in the `stringr` library, where we've gotten `str_count` and `str_replace_all`. We'll come back to some of those, but for now, let's jump into a bit more analysis by shifting to working with the `tidytext` library. 

First, we're going to shift our data into a tidy format.

```{r}
tidy_interviews <- interviews %>%
  unnest_tokens(word, text) %>%
  select(doc_id, word)

tidy_interviews
```

What's happenened to our data? What changes do you see?

At the same time that the `unnest_tokens` function tokenizes the text, it also lowercases everything and removes punctuation. We can set some parameters to affect this if we don't want all of that to happen. 

```{r}
interviews %>%
  unnest_tokens(word, text, to_lower = FALSE, strip_punct = FALSE)
```

We can also tokenize at different levels other than words, such as sentences or ngrams. 

```{r}
interviews %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  select(doc_id, sentence)
```

```{r}
interviews %>%
  unnest_tokens(bigrams, text, token = "ngrams", n = 2) %>%
  select(doc_id, bigrams)
```

We're going to stick with our word level tokenization for now, and keep moving. Once of the most common things we do in text analysis is just counting words. Working with our text in a tidy fashion makes that easy.

```{r}
tidy_interviews %>% 
  count(word)
```

One of the first things we notice is that there are a lot of numbers. We could pull those out if we want, but let's leave them for the moment in case some pop up with high frequency and we want to investigate. Let's reorder the counts though so we can see the most common words. 

```{r}
tidy_interviews %>% 
  count(word) %>%
  arrange(desc(n))
```

The results here shouldn't be too surprising. These are interviews, and whether we're speaking or writing, we often use lots of words like "the", "of", or "a". Let's remove these from our corpus so we have less noise. This is typically called stopword removal. 

Tidytext provides a list of these common stopwords that we can use, though you can always define your own or modify an existing list. 

```{r}
stop_words
```

Since we're working in a tidyverse way, and both our data and these stopwords are in tibbles, we can use an `anti_join` on the two dataframes to return a tibble that contains all rows when there aren't matching values in the stopwords. Because we have the column "word" in both tibbles, we don't have to specify it. 

```{r}
tidy_interviews_clean <- tidy_interviews %>%
  anti_join(stop_words)

tidy_interviews_clean
```

Note that we went from almost 360,000 rows to only 120,000. That's a big reduction. Let's rerun our counts now. Instead of using `arrange`, which we used in earlier workshops, we can actually use a parameter in the `count` function.

```{r}
tidy_interviews_clean %>% 
  count(word, sort = TRUE) 
```

What do you see here? And what would you consider doing about it?

For now, let's not worry too much about it. We'll proceed with some visualiation and analysis and then figure out whether we need to go back and clean further. 

Since graphs are always nice, let's make a quick graph with ggplot to show these most frequent words. We do have to use `mutate` and `reorder` to make sure that ggplot respects the sorting of the words.

```{r}
tidy_interviews_clean %>% 
  count(word) %>%
  filter(n > 500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(fill = "gray") +
  coord_flip() + # This helps with reading the words.
  ggtitle("Words mentioned more than 500 times")
```

If we were interested in just a single interview, we could always filter down to that and then graph. 

```{r}
tidy_interviews_clean %>% 
  filter(doc_id == "AndresenRuth_1stInterview_transcript.txt") %>%
  count(word) %>%
  filter(!word %in% c("andresen", "ruth", "john", "tim", "thomas")) %>%
  filter(n > 10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(fill = "gray") +
  coord_flip()
```

Computational text analysis doesn't always mean working with big data or large corpora at scale. Sometimes it's just a useful way to engage with individual documents, and understand some of the emphasis in each by looking at it just a bit differently.

While the raw word counts are useful, within the context of a corpus, we often want to scale the word counts according to the number of words in each document. This gives us the term frequency, and a better sense of the importance of words per document. 

```{r}
tidy_interviews_clean %>%
  count(doc_id, word, sort = TRUE)  %>%  # count words for each doc
  group_by(doc_id) %>% # group docs together
  mutate(n_tot = sum(n), # count total number of words per doc
         term_freq = n/n_tot)
```

We can see that the names are really starting to be a bit of problem, so let's remove them. 

```{r}
split_names <- function(name) {
  chunks <- str_split_fixed(name, "_", 2)
  mod_string <- gsub("([[:upper:]])", " \\1", chunks[,1])
  first_last <- str_split_fixed(str_trim(mod_string, side="left"), " ", 2)
  return(c(first_last))
}

split_names("SchroederPaul_transcript.txt")

names <- map(interviews$doc_id, split_names)
names_flat <- unlist(names)
names_flat_lower <- unlist(map(names_flat, str_to_lower))
names_flat_lower
```

```{r}
tidy_interviews_clean <- tidy_interviews_clean %>%
  filter(!word %in% names_flat_lower)

tidy_interviews_clean
```



TODO: make data frame from documents where each interview chunk is a row in the dataframe.

