---
title: "Text analysis in R"
author: "Scott Bailey"
date: "7/23/2019"
output:
  pdf_document: default
  word_document: default
---

The content of this workshop is heavily depending on a workshop that Claudia Engel and I put together and taught in Spring 2019, and which Claudia has further refined: https://cengel.github.io/R-text-analysis/. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, results="hide")
```

```{r}
library(tidyverse)
library(tidytext)
library(readtext)
library(tm)
library(topicmodels)
```

The texts we're going to work with are a set of interviews from here at Hopkins. You can download them from here: 

During our workshop, we're going to work through a standard workflow in text analysis: read in texts, organize them, clean them, and analyze them.

We'll use a special package, `readtext`, to read the texts in by passing in the directory.

```{r}
interviews_raw <- readtext("plain_text")

head(interviews_raw)
```

To understanding how we need to clean the texts, we need to take a close look at one of them. One of the first things we can see is that the text is extraordinarily clean. It's re-keyed text or a corrected transcript, rather than OCR, so we don't need to clean or handle misspellings or strange characters having come in by way of the OCDR process. 

We should also notice that there is front-matter to teach text file, with information about the interview. If we look at several of these, we'll see that they take a different number of lines in different files, which is important.If we're wanting to do semantic analysis of the text, though, we probably want to pull out the front matter, though if you have a large enough corpus some of that information would wash out in the analysis. 

The last thing to observe is that, for the most part, each line in the file represents a switch in person speaking, and each line has the person speaking identified at the front of the line. This isn't always the case, but it helps we want to analyze these files in terms of what the interview says vs what the interviewee says. 

Let's start by splitting the front-matter from the interview itself. Can anyone suggest a way to do this?

```{r}
interviews <- interviews_raw %>%
  separate(text, c("series_title", "front_matter", "text"), "\n\n", extra="merge")

head(interviews)
```

Separate allows us to split a column into multiple columns based on some character pattern. We have to specific the `extra` parameter so that we control what happens when separate would create more columns than we want, i.e., when there are more characters that satisfy our pattern than we want.

Now we can do a bit of cleaning. Let's remove the word "INTERVIEW" since that doesn't meaningfully contribute to our analysis. 

```{r}
interviews <- interviews %>%
  mutate(text = str_replace_all(text, "INTERVIEW", ""))

head(interviews)
```

Since the Bill Austin interview is notes from the interview rather than a transcript, let's drop it from our analysis. We want our documents to all be roughly the same in format. 


```{r}
interviews <- interviews %>%
  filter(!doc_id == "AustinBill_phoneInterview.txt")

head(interviews)
```

Now that we have our data mostly standardized, let's start asking some questions about it. 

How many times is Hopkins mentioned in each interview?

```{r}
interviews %>%
  pull(text) %>% # pull extracts just that column vector
  str_count("Hopkins")
```

We got back a vector here, so we could assign that to a variable and do whatever calculations on it we want to. Or, we could create a new column in our dataset with that information for any words we are particularly interested in.

Let's do something smiilar, but with how many words are in each interview.

```{r}
interviews <- interviews %>%
  mutate(n_words = str_count(text, boundary("word")))

head(interviews)
```

The `boundary` parameter lets us choose some time of entity to count by. Along with words, it can count thinks like characters, sentences, and line breaks. 

There are quite a function string functions in the `stringr` library, where we've gotten `str_count` and `str_replace_all`. We'll come back to some of those, but for now, let's jump into a bit more analysis by shifting to working with the `tidytext` library. 

First, we're going to shift our data into a tidy format.

```{r}
tidy_interviews <- interviews %>%
  unnest_tokens(word, text) %>%
  select(doc_id, word)

tidy_interviews
```

What's happenened to our data? What changes do you see?

At the same time that the `unnest_tokens` function tokenizes the text, it also lowercases everything and removes punctuation. We can set some parameters to affect this if we don't want all of that to happen. 

```{r}
interviews %>%
  unnest_tokens(word, text, to_lower = FALSE, strip_punct = FALSE)
```

We can also tokenize at different levels other than words, such as sentences or ngrams. 

```{r}
interviews %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  select(doc_id, sentence)
```

```{r}
interviews %>%
  unnest_tokens(bigrams, text, token = "ngrams", n = 2) %>%
  select(doc_id, bigrams)
```

We're going to stick with our word level tokenization for now, and keep moving. Once of the most common things we do in text analysis is just counting words. Working with our text in a tidy fashion makes that easy.

```{r}
tidy_interviews %>% 
  count(word)
```

One of the first things we notice is that there are a lot of numbers. We could pull those out if we want, but let's leave them for the moment in case some pop up with high frequency and we want to investigate. Let's reorder the counts though so we can see the most common words. 

```{r}
tidy_interviews %>% 
  count(word) %>%
  arrange(desc(n))
```

The results here shouldn't be too surprising. These are interviews, and whether we're speaking or writing, we often use lots of words like "the", "of", or "a". Let's remove these from our corpus so we have less noise. This is typically called stopword removal. 

Tidytext provides a list of these common stopwords that we can use, though you can always define your own or modify an existing list. 

```{r}
stop_words
```

Since we're working in a tidyverse way, and both our data and these stopwords are in tibbles, we can use an `anti_join` on the two dataframes to return a tibble that contains all rows when there aren't matching values in the stopwords. Because we have the column "word" in both tibbles, we don't have to specify it. 

```{r}
tidy_interviews_clean <- tidy_interviews %>%
  anti_join(stop_words)

tidy_interviews_clean
```

Note that we went from almost 360,000 rows to only 120,000. That's a big reduction. Let's rerun our counts now. Instead of using `arrange`, which we used in earlier workshops, we can actually use a parameter in the `count` function.

```{r}
tidy_interviews_clean %>% 
  count(word, sort = TRUE) 
```

What do you see here? And what would you consider doing about it?

For now, let's not worry too much about it. We'll proceed with some visualiation and analysis and then figure out whether we need to go back and clean further. 

Since graphs are always nice, let's make a quick graph with ggplot to show these most frequent words. We do have to use `mutate` and `reorder` to make sure that ggplot respects the sorting of the words.

```{r}
tidy_interviews_clean %>% 
  count(word) %>%
  filter(n > 500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(fill = "gray") +
  coord_flip() + # This helps with reading the words.
  ggtitle("Words mentioned more than 500 times")
```

If we were interested in just a single interview, we could always filter down to that and then graph. 

```{r}
tidy_interviews_clean %>% 
  filter(doc_id == "AndresenRuth_1stInterview_transcript.txt") %>%
  count(word) %>%
  filter(!word %in% c("andresen", "ruth", "john", "tim", "thomas")) %>%
  filter(n > 10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(fill = "gray") +
  coord_flip()
```

Computational text analysis doesn't always mean working with big data or large corpora at scale. Sometimes it's just a useful way to engage with individual documents, and understand some of the emphasis in each by looking at it just a bit differently.

While the raw word counts are useful, within the context of a corpus, we often want to scale the word counts according to the number of words in each document. This gives us the term frequency, and a better sense of the importance of words per document. 

```{r}
tidy_interviews_clean %>%
  count(doc_id, word, sort = TRUE)  %>%  # count words for each doc
  group_by(doc_id) %>% # group docs together
  mutate(n_tot = sum(n), # count total number of words per doc
         term_freq = n/n_tot)
```

We can see that the names are really starting to be a bit of problem, so let's remove them. 

```{r}
split_names <- function(name) {
  chunks <- str_split_fixed(name, "_", 2)
  mod_string <- gsub("([[:upper:]])", " \\1", chunks[,1])
  first_last <- str_split_fixed(str_trim(mod_string, side="left"), " ", 2)
  return(c(first_last))
}

split_names("SchroederPaul_transcript.txt")

names <- map(interviews$doc_id, split_names)
names_flat <- unlist(names)
names_flat_lower <- unlist(map(names_flat, str_to_lower))
names_flat_lower
```

```{r}
tidy_interviews_clean <- tidy_interviews_clean %>%
  filter(!word %in% names_flat_lower)

tidy_interviews_clean
```

Let's rerun our code to determine term frequencies:

```{r}
tidy_interviews_clean %>%
  count(doc_id, word, sort = TRUE)  %>%  
  group_by(doc_id) %>% 
  mutate(n_tot = sum(n),
         term_freq = n/n_tot) %>%
  arrange(desc(term_freq))
```

We still have names here, but these are at least names mentioned by the interviewees rather than the names of the interviewees occuring at least in part because of name at the front of each line.

Let's graph the distribution of words for a single document using a histogram. 

```{r}
tidy_interviews_clean %>%
  filter(doc_id == "SilbersteinMark_transcript.txt") %>%
  count(doc_id, word)  %>%  # count n for each word
  group_by(doc_id) %>% 
  mutate(n_tot = sum(n), # count total number of words per doc
         term_freq = n/n_tot) %>% 
  ggplot(aes(term_freq)) +
    geom_histogram() 
```

This is what almost any distribution for a text will look like. Only a few words have a high term frequency, i.e., are highly used in that text. Most words are used in small numbers. 

Term frequencies are helpful for us to understand what words might be important for a text. What if we want to understand what words are important for a text within the context of a corpus? For that, we can use tf-idf, or term frequency inverse document frequency. Tf-idf measures how important a word is within a corpus by scaling term frequency per document according to the inverse of the term’s document frequency (number of documents within the corpus in which the term appears divided by the number of documents).

The tf-idf value will be:

- lower for words that appear in many documents in the corpus, and lowest when the word occurs in virtually all documents.
- high for words that appear many times in few documents in the corpus, this lending high discriminatory power to those documents.

Since tf-idf is a common measure, `tidytext` supplies a convenient function for it. 

```{r}
tidy_interview_tfidf <- tidy_interviews_clean %>%
  count(doc_id, word, sort = TRUE)  %>%  # aggregate to count n for each word
  bind_tf_idf(word, doc_id, n) 

tidy_interview_tfidf
```

You'll notice that in the process of calculating the tf-idf score for each word in each doc, the `bind_tf_idf` function also calculates the term frequency and inverse document frequency, which are necessary pieces. 

Let's see which words in the corpus have the highest tf-idf scores. 

```{r}
tidy_interview_tfidf %>%
  arrange(desc(tf_idf))
```

We still get quite a few names, but we can get a sense now of which words particular interviewees used much more than other interviewees. It might give us some information that we could use to follow up in the specific interview or that might help us understand prominent topics during a time period. 

Let's look at how this tidytext approach can open to other analytical tools like topic modeling. 

First, let's turn our corpus into a document term matrix. A DTM is a standard representation of a text as a matrix where each row represents a document, and each column represents a feature, in this case a word. The value at any given spot is typically the number of times that word occurs in the document. You can also build a document frequency matrix, which is the same but with a frequency rather than account. The types of representations are often used for machine learning. 

Since this is fairly normal, `tidytext` provides a function to easily convert a tibble with doc_id, the word, and the count into a DTM. 

```{r}
interviews_dtm <- tidy_interviews_clean %>%
  count(doc_id, word) %>%
  cast_dtm(doc_id, word, n)

interviews_dtm
```

Once we have a DTM, we can use just a lot of functions from the `tm` package, such as `findAssoc`, which lets us find correlated terms within a limit. 

```{r}
findAssocs(interviews_dtm, "marine", corlimit = 0.5)
findAssocs(interviews_dtm, "science", corlimit = 0.5)
```

We don't really have enough texts for it, but with a DTM we can start to use topic modeling, which is, usually, an unsupervised machine learning model for uncovering latent semantic structures in a corpus. 

```{r}
# code adapted from https://www.tidytextmining.com/topicmodeling.html

interviews_lda <- LDA(interviews_dtm, k=10)

interview_topics <- tidy(interviews_lda, matrix = "beta")

interviews_top_terms <- interview_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

head(interviews_top_terms)

interviews_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

Our results here aren't great, because we have a small corpus and the interviews themselves may have a large amount of topical overlap. The idea, though, is that once we can convert our text corpus into certain formats, we're able to use that data for different types of modeling that can be really interesting. 

